{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device = 'mps'\n",
    "print(device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Preparation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First define where the training data url is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "cwd = os.getcwd()\n",
    "repository_dir = '/'.join(cwd.split('/')[:-1])\n",
    "# DATA_DIR = f'{repository_dir}/data/data/preprocessed/preprocessed.txt'\n",
    "DATA_DIR = 'input.txt'\n",
    "assert os.path.exists(DATA_DIR), 'Make sure you follow the steps of README.md in data repository'\n",
    "\n",
    "print(f'Training data located at {DATA_DIR}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(DATA_DIR, 'r', encoding='utf-8') as train_file:\n",
    "    raw_data = train_file.read()\n",
    "\n",
    "\n",
    "# Inspection\n",
    "print(f'Loaded {len(raw_data)} characters')\n",
    "print(f'First 100 characters:\\n{raw_data[:100]}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenization is essentially turning the data into 'words' that the model understands. Below the vocabulary that is defined is based on a character basis. \n",
    "\n",
    "TODO: check out other tokenizers:\n",
    "tiktoken - https://github.com/openai/tiktoken\n",
    "sentencepiece - https://github.com/google/sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = sorted(list(set(raw_data)))\n",
    "VOCAB_SIZE = len(chars)\n",
    "print(f'Vocabulary ({VOCAB_SIZE} characters): {\"\".join(chars)}')\n",
    "char_to_digit = {char:digit for digit, char in enumerate(chars)}\n",
    "digit_to_char = {digit:char for digit, char in enumerate(chars)}\n",
    "\n",
    "encode = lambda string: [char_to_digit[char] for char in string]\n",
    "decode = lambda digits: ''.join([digit_to_char[digit] for digit in digits])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch # we use PyTorch: https://pytorch.org\n",
    "data = torch.tensor(encode(raw_data), dtype=torch.long)\n",
    "print(data.shape, data.dtype)\n",
    "print(data[:1000]) # the 1000 characters we looked at earier will to the GPT look like this"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train/Val Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_PROPORTION = 0.9 # 90% of data is for training, 10% for validation\n",
    "idx = int(TRAIN_PROPORTION * len(data)) \n",
    "train_data = data[:idx]\n",
    "val_data = data[idx:]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chunking data\n",
    "\n",
    "It is important to note that it is computationaly infeasible and prohibitive to train on the **whole** dataset all at once, due to the large training data size. This is why data is separated into *chunks*, which are smaller random samples of the whole dataset. The size of the chunks is defined by the variable `block_size`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BLOCK_SIZE = 64\n",
    "train_data[:BLOCK_SIZE+1]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The way training is carried out is that for every character `c` in a chunk, `c` is the label and the inputs are all characters before `c`. \n",
    "This approach is useful, because the model can start predicting from as little as a single character, and predict everything up until block size\n",
    "#### Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = \"Hello, there\"\n",
    "print(f\"Input{' ' * 7}| Output\")\n",
    "print('='*22)\n",
    "for t in range(1, len(example)):\n",
    "    inputs = example[:t]\n",
    "    output = example[t]\n",
    "\n",
    "    print(f\"{inputs:<11} | {output}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch\n",
    "\n",
    "There is an additional dimension except the time dimension (blocks), which is the batch. Multiple blocks are sampled and stacked on top of each other to create a batch. This way multiple samples can be processed in parallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "BLOCK_SIZE = 128\n",
    "SEED = 1338\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "def get_batch(split = 'train'):\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    # data = torch.tensor([i for i in range(100)])\n",
    "\n",
    "    # Generate BATCH_SIZE random starting positions for each block\n",
    "    block_indices = torch.randint(len(data) - BLOCK_SIZE, (BATCH_SIZE,)) \n",
    "    block_range = torch.arange(0, BLOCK_SIZE, dtype=torch.int)\n",
    "\n",
    "    # Add the above range to each starting position to create a set\n",
    "    # of indices for each block \n",
    "    block_ranges = block_indices[:, None] + block_range\n",
    "    \n",
    "    x = data[block_ranges] \n",
    "    y = data[block_ranges + 1]\n",
    "    return x.to(device), y.to(device)    \n",
    "\n",
    "print(get_batch())\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss(model, eval_iters = 100):\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    \n",
    "    for split in [\"test\", \"validation\"]:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            x, y = get_batch(split)\n",
    "            loss = model(x, y)\n",
    "            losses[k] = loss\n",
    "\n",
    "        out[split] = losses.mean().item()\n",
    "\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bigram Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "\n",
    "    def predict(self, input):\n",
    "        return self.token_embedding_table(input)\n",
    "\n",
    "    def forward(self, input, targets):\n",
    "        logits = self.predict(input) # logits.shape = B, T, C - Batch, Time, Channels\n",
    "        B, T, C = logits.shape\n",
    "        transformed_logits = logits.view(B*T, C)\n",
    "        targets = targets.view(B*T)\n",
    "        loss = F.cross_entropy(transformed_logits, targets) # Expects B*T, C\n",
    "        return loss\n",
    "    \n",
    "    def generate(self, input, max_characters=20):\n",
    "        for _ in range(max_characters):\n",
    "            logits = self.predict(input)\n",
    "            logits = logits[:, -1, :]\n",
    "            distribution = F.softmax(logits, dim=-1)\n",
    "            next = torch.multinomial(distribution, num_samples=1)\n",
    "            input = torch.cat((input, next), dim=1)\n",
    "        return input[0]\n",
    "\n",
    "model = BigramLanguageModel(VOCAB_SIZE).to(device)\n",
    "x, y = get_batch()\n",
    "\n",
    "context = torch.zeros((1, 1), dtype=torch.long).to(device)\n",
    "res = model.generate(context)\n",
    "print(decode(res.tolist()))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create an optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
    "# https://arxiv.org/abs/1711.05101v3\n",
    "# https://paperswithcode.com/method/adamw"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import time\n",
    "\n",
    "def train(epochs = 100000):\n",
    "\n",
    "    loss_estimation = 10000\n",
    "    start = time.time()\n",
    "\n",
    "    for i in range(epochs):\n",
    "        x, y = get_batch()\n",
    "        loss = model.forward(x, y)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        # print(loss)\n",
    "        optimizer.step()\n",
    "        if i % loss_estimation == 0:\n",
    "            print(f'Epoch {i} - {estimate_loss(model)}')\n",
    "            end = time.time()\n",
    "            print(end - start)\n",
    "            start = time.time()\n",
    "\n",
    "    \n",
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = torch.zeros((1, 1), dtype=torch.long).to(device)\n",
    "res = model.generate(context, max_characters=200)\n",
    "print(decode(res.tolist()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fyp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "faf7ff0063859ba4ac17d2ca41341beebe28095055f136698c75e8f10562c7e6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
