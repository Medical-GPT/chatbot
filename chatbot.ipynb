{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device = 'mps'\n",
    "print(device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Preparation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First define where the training data url is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "cwd = os.getcwd()\n",
    "repository_dir = '/'.join(cwd.split('/')[:-1])\n",
    "# DATA_DIR = f'{repository_dir}/data/data/preprocessed/preprocessed.txt'\n",
    "DATA_DIR = 'input.txt'\n",
    "assert os.path.exists(DATA_DIR), 'Make sure you follow the steps of README.md in data repository'\n",
    "\n",
    "print(f'Training data located at {DATA_DIR}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(DATA_DIR, 'r', encoding='utf-8') as train_file:\n",
    "    raw_data = train_file.read()\n",
    "\n",
    "\n",
    "# Inspection\n",
    "print(f'Loaded {len(raw_data)} characters')\n",
    "print(f'First 100 characters:\\n{raw_data[:100]}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenization is essentially turning the data into 'words' that the model understands. Below the vocabulary that is defined is based on a character basis. \n",
    "\n",
    "TODO: check out other tokenizers:\n",
    "tiktoken - https://github.com/openai/tiktoken\n",
    "sentencepiece - https://github.com/google/sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = sorted(list(set(raw_data)))\n",
    "VOCAB_SIZE = len(chars)\n",
    "print(f'Vocabulary ({VOCAB_SIZE} characters): {\"\".join(chars)}')\n",
    "char_to_digit = {char:digit for digit, char in enumerate(chars)}\n",
    "digit_to_char = {digit:char for digit, char in enumerate(chars)}\n",
    "\n",
    "encode = lambda string: [char_to_digit[char] for char in string]\n",
    "decode = lambda digits: ''.join([digit_to_char[digit] for digit in digits])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch # we use PyTorch: https://pytorch.org\n",
    "data = torch.tensor(encode(raw_data), dtype=torch.long)\n",
    "print(data.shape, data.dtype)\n",
    "print(data[:1000]) # the 1000 characters we looked at earier will to the GPT look like this"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train/Val Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_PROPORTION = 0.9 # 90% of data is for training, 10% for validation\n",
    "idx = int(TRAIN_PROPORTION * len(data)) \n",
    "train_data = data[:idx]\n",
    "val_data = data[idx:]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chunking data\n",
    "\n",
    "It is important to note that it is computationaly infeasible and prohibitive to train on the **whole** dataset all at once, due to the large training data size. This is why data is separated into *chunks*, which are smaller random samples of the whole dataset. The size of the chunks is defined by the variable `block_size`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BLOCK_SIZE = 64\n",
    "train_data[:BLOCK_SIZE+1]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The way training is carried out is that for every character `c` in a chunk, `c` is the label and the inputs are all characters before `c`. \n",
    "This approach is useful, because the model can start predicting from as little as a single character, and predict everything up until block size\n",
    "#### Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = \"Hello, there\"\n",
    "print(f\"Input{' ' * 7}| Output\")\n",
    "print('='*22)\n",
    "for t in range(1, len(example)):\n",
    "    inputs = example[:t]\n",
    "    output = example[t]\n",
    "\n",
    "    print(f\"{inputs:<11} | {output}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch\n",
    "\n",
    "There is an additional dimension except the time dimension (blocks), which is the batch. Multiple blocks are sampled and stacked on top of each other to create a batch. This way multiple samples can be processed in parallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "BLOCK_SIZE = 128\n",
    "N_EMBD = 64\n",
    "SEED = 1338\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "def get_batch(split = 'train'):\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    # data = torch.tensor([i for i in range(100)])\n",
    "\n",
    "    # Generate BATCH_SIZE random starting positions for each block\n",
    "    block_indices = torch.randint(len(data) - BLOCK_SIZE, (BATCH_SIZE,)) \n",
    "    block_range = torch.arange(0, BLOCK_SIZE, dtype=torch.int)\n",
    "\n",
    "    # Add the above range to each starting position to create a set\n",
    "    # of indices for each block \n",
    "    block_ranges = block_indices[:, None] + block_range\n",
    "    \n",
    "    x = data[block_ranges] \n",
    "    y = data[block_ranges + 1]\n",
    "    return x.to(device), y.to(device)    \n",
    "\n",
    "print(get_batch())\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss(model, eval_iters = 10):\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    \n",
    "    for split in [\"test\", \"validation\"]:\n",
    "        losses = torch.zeros(eval_iters).to(device)\n",
    "        for k in range(eval_iters):\n",
    "            x, y = get_batch(split)\n",
    "            loss = model(x, y)\n",
    "            losses[k] = loss\n",
    "\n",
    "        out[split] = losses.mean().item()\n",
    "\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bigram Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "\n",
    "    def predict(self, input):\n",
    "        return self.token_embedding_table(input)\n",
    "\n",
    "    def forward(self, input, targets):\n",
    "\n",
    "        # Number of channels is the same as the vocabulary size meaning for each letter\n",
    "        # we have a vector of size VOCAB_SIZE storing the probability of each letter appearing\n",
    "        logits = self.predict(input) # logits.shape = B, T, C - Batch, Time, Channels\n",
    "        B, T, C = logits.shape\n",
    "        transformed_logits = logits.view(B*T, C)\n",
    "        targets = targets.view(B*T)\n",
    "        loss = F.cross_entropy(transformed_logits, targets) # Expects B*T, C\n",
    "        return loss\n",
    "    \n",
    "    def generate(self, input, max_characters=20):\n",
    "        for _ in range(max_characters):\n",
    "            logits = self.predict(input)\n",
    "            logits = logits[:, -1, :]\n",
    "            distribution = F.softmax(logits, dim=-1)\n",
    "            next = torch.multinomial(distribution, num_samples=1)\n",
    "            input = torch.cat((input, next), dim=1)\n",
    "        return input[0]\n",
    "\n",
    "# model = BigramLanguageModel(VOCAB_SIZE).to(device)\n",
    "# x, y = get_batch()\n",
    "\n",
    "# context = torch.zeros((1, 1), dtype=torch.long).to(device)\n",
    "# res = model.generate(context)\n",
    "# print(decode(res.tolist()))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def train(model, epochs = 10000, learning_rate = 1e-3, eval_step = 1000):\n",
    "    # https://arxiv.org/abs/1711.05101v3\n",
    "    # https://paperswithcode.com/method/adamw\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr = learning_rate)\n",
    "    start = time.time()\n",
    "\n",
    "    for i in range(epochs):\n",
    "        x, y = get_batch()\n",
    "        loss = model(x, y)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if i % eval_step == 0:\n",
    "            print(f'Epoch {i} - {estimate_loss(model)}')\n",
    "            end = time.time()\n",
    "            print(f\"Time taken: {end - start}\")\n",
    "            start = time.time()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Self-attention \n",
    "Taking into consideration all previous characters."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by just taking the average of the previous characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "B, T, C = 4, 8, 2\n",
    "\n",
    "x = torch.randn(B, T, C)\n",
    "\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Version 1\n",
    "\n",
    "import torch\n",
    "\n",
    "mask = torch.tril(torch.ones(T, T)) # returns the lower triangle of a matrix\n",
    "mask /= mask.sum(axis=1, keepdim=True) # normalize the mask so each row sums to 1\n",
    "\n",
    "print(mask @ x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Version 2\n",
    "# Using softmax to normalize the mask in order to directly be able to use the embedding table\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "mask = torch.tril(torch.ones(T, T)) # returns the lower triangle of a matrix\n",
    "affinities = torch.ones(T, T)\n",
    "affinities = affinities.masked_fill(mask == 0, float('-inf'))\n",
    "affinities = F.softmax(affinities, dim=1)\n",
    "\n",
    "print(affinities @ x)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single Head to perform self-attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "B, T, C = 4, 8, 2\n",
    "x = torch.randn(B, T, C)\n",
    "\n",
    "# Head of self-attention\n",
    "head_size = 16\n",
    "keys = nn.Linear(C, head_size, bias=False)  # What is stored\n",
    "query = nn.Linear(C, head_size, bias=False) # What is requied\n",
    "value = nn.Linear(C, head_size, bias=False) \n",
    "k = keys(x)  # B, T, head_size\n",
    "q = query(x) # B, T, head_size\n",
    "\n",
    "v = value(x) # Multiply the affinities by this value instead of x\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An intuitive way to think about the query is that for example for a vowel a consonant 2 positions earlier might be more important and this is reflected using the query. \n",
    "In other words it is a way to weight the positions in a data dependent manner. The keys layer on the other hand shows what is contained as weights. In the case of the previous example, if it is important that there is a consonant 2 letters before and there is infact a consonant the dot product will yield a high affinitie."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating the affinities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the affinities to the dot product of the keys and queries instead of to a torch of ones\n",
    "# The affinities are a weighted average where the weights are defined by the query and the \n",
    "# actual values are defined by the keys\n",
    "affinities = k @ q.transpose(-1, -2) # (B, T, head_size) @ (B, head_size, T) = (B, T, T)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem with just mutliplying the keys by the queries will yield variance of the order of \n",
    "`head_size` when weights are initialized. Why?\n",
    "\n",
    "(Attention is all you need https://arxiv.org/abs/1706.03762)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "B, T, head_size = 4, 8, 15\n",
    "sample_size = 100\n",
    "\n",
    "k_var = 0\n",
    "q_var = 0\n",
    "affinities_var = 0\n",
    "\n",
    "for _ in range(sample_size):\n",
    "    k = torch.randn(B, T, head_size)\n",
    "    q = torch.randn(B, T, head_size)\n",
    "    affinities = k @ q.transpose(-1, -2)\n",
    "\n",
    "    k_var += k.var()\n",
    "    q_var += q.var()\n",
    "    affinities_var += affinities.var()\n",
    "\n",
    "print(k_var / sample_size)\n",
    "print(q_var / sample_size)\n",
    "print(affinities_var / sample_size)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is because rand initializes the tensor with numbers drawn from a gaussian distribution (i.e. they have a 0-mean and a 1-variance). \n",
    "This means that the dot product of matrix where each row/column is a vector with 0-mean and 1-variance will result in the summing up of `head_size` number of vectors with 1-variance and the vectors in the resulting matrix will have `1 * head_size` variance. See proof below\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_size = 100000\n",
    "sum = 0\n",
    "head_size = 3\n",
    "for _ in range(sample_size):\n",
    "    a = torch.randn(head_size, 3)\n",
    "    b = torch.randn(head_size, 3)\n",
    "\n",
    "    sum += (a @ b * head_size ** -0.5).var()\n",
    "\n",
    "print(sum / sample_size)\n",
    "\n",
    "\n",
    "# k = sqrt(head_size)\n",
    "# sigma_a = E[a^2 / k^2] = E[a^2] / head_size\n",
    "# \n",
    "# sigma_t = sigma_a + sigma_b = (E[a^2] + E[b^2]) / head_size\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Motivation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reason why it is important to normalize the weights (especially in the initial training) is that the softmax function converges to one-hot encoding of the largest value when there are very positive and very negative values in the same vector. See example below. This means that without the normalization information would be aggregated only from the highest-valued node which loses a lot of information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.randn(5)\n",
    "sharp_a = a * 10\n",
    "print(F.softmax(a, dim=0)) # Values are fairly diffuse\n",
    "print((F.softmax(sharp_a, dim=0, dtype=torch.float32) * 10000).round()/10000) # Approximates 1-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = torch.tril(torch.ones(T, T)) \n",
    "affinities = affinities.masked_fill(mask == 0, float('-inf'))\n",
    "affinities = F.softmax(affinities, dim=-1)\n",
    "\n",
    "print(v.shape)\n",
    "print(affinities.shape)\n",
    "print((affinities @ v).shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bringing everything together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(N_EMBD, head_size, bias=False)\n",
    "        self.query = nn.Linear(N_EMBD, head_size, bias=False)\n",
    "        self.value = nn.Linear(N_EMBD, head_size, bias=False)\n",
    "        # Not a parameter of the module, so a pytorch naming convention\n",
    "        self.register_buffer(\"mask\", torch.tril(torch.ones(BLOCK_SIZE, BLOCK_SIZE)))\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        B, T, C = inputs.shape\n",
    "        k = self.key(inputs)\n",
    "        q = self.query(inputs)\n",
    "        v = self.value(inputs)\n",
    "        \n",
    "        affinities = k @ q.transpose(-1, -2) * (C ** -0.5)\n",
    "        affinities = affinities.masked_fill(self.mask[:T, :T] == 0, float('-inf'))\n",
    "        affinities = F.softmax(affinities, dim=1)\n",
    "\n",
    "        return affinities @ v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BigramLanguageModelSelfAttention(nn.Module):\n",
    "   \n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, N_EMBD)\n",
    "        self.position_embedding_table = nn.Embedding(BLOCK_SIZE, N_EMBD)\n",
    "        self.sa_head = Head(N_EMBD)\n",
    "        self.lm_head = nn.Linear(N_EMBD, vocab_size)\n",
    "\n",
    "    def predict(self, input):\n",
    "        B, T = input.shape\n",
    "        tok_embd = self.token_embedding_table(input)\n",
    "        pos_embd = self.position_embedding_table(torch.arange(T, device=device))\n",
    "        x = tok_embd + pos_embd\n",
    "\n",
    "        x = self.sa_head(x)\n",
    "        x = self.lm_head(x)\n",
    "        return x\n",
    "\n",
    "    def forward(self, input, targets):\n",
    "        logits = self.predict(input)\n",
    "        B, T, C = logits.shape\n",
    "        logits = logits.view(B*T, C)\n",
    "        targets = targets.view(B*T)\n",
    "        loss = F.cross_entropy(logits, targets)\n",
    "        return loss\n",
    "\n",
    "    def generate(self, input, max_characters=20):\n",
    "        for _ in range(max_characters):\n",
    "            # Crop the input to the last BLOCK_SIZE tokens\n",
    "            input_cond = input[:, -BLOCK_SIZE:]\n",
    "\n",
    "            logits = self.predict(input_cond)\n",
    "            logits = logits[:, -1, :]\n",
    "            distribution = F.softmax(logits, dim=-1)\n",
    "            next = torch.multinomial(distribution, num_samples=1)\n",
    "            input = torch.cat((input_cond, next), dim=1)\n",
    "        return input_cond[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Head attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, n_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(n_heads)])\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        B, T, C = inputs.shape\n",
    "        x = torch.cat([head(inputs) for head in self.heads], dim=-1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BigramLanguageModelMultiHeadAttention(nn.Module):\n",
    "   \n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, N_EMBD)\n",
    "        self.position_embedding_table = nn.Embedding(BLOCK_SIZE, N_EMBD)\n",
    "        # Create 4 heads with size N_EMBD / 4, works like group convolution, where each head corresponds to a part of an image\n",
    "        self.sa_heads = MultiHeadAttention(4, N_EMBD // 4)\n",
    "\n",
    "        self.lm_head = nn.Linear(N_EMBD, vocab_size)\n",
    "\n",
    "    def predict(self, input):\n",
    "        B, T = input.shape\n",
    "        tok_embd = self.token_embedding_table(input)\n",
    "        pos_embd = self.position_embedding_table(torch.arange(T, device=device))\n",
    "        x = tok_embd + pos_embd\n",
    "\n",
    "        x = self.sa_heads(x)\n",
    "        x = self.lm_head(x)\n",
    "        return x\n",
    "\n",
    "    def forward(self, input, targets):\n",
    "        logits = self.predict(input)\n",
    "        B, T, C = logits.shape\n",
    "        logits = logits.view(B*T, C)\n",
    "        targets = targets.view(B*T)\n",
    "        loss = F.cross_entropy(logits, targets)\n",
    "        return loss\n",
    "\n",
    "    def generate(self, input, max_characters=20):\n",
    "        for _ in range(max_characters):\n",
    "            # Crop the input to the last BLOCK_SIZE tokens\n",
    "            input_cond = input[:, -BLOCK_SIZE:]\n",
    "\n",
    "            logits = self.predict(input_cond)\n",
    "            logits = logits[:, -1, :]\n",
    "            distribution = F.softmax(logits, dim=-1)\n",
    "            next = torch.multinomial(distribution, num_samples=1)\n",
    "            input = torch.cat((input_cond, next), dim=1)\n",
    "        return input_cond[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feed-forward "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, n_features):\n",
    "        super().__init__()\n",
    "        # self.net = nn.Sequential(\n",
    "        #     nn.Linear(n_features, 4 * n_features),\n",
    "        #     nn.GELU(),\n",
    "        #     nn.Linear(4 * n_features, n_features),\n",
    "        # )\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_features, 4*n_features),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4*n_features, n_features),\n",
    "        )\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        return self.net(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BigramLanguageModelMultiHeadAttentionFeedForward(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, N_EMBD)\n",
    "        self.position_embedding_table = nn.Embedding(BLOCK_SIZE, N_EMBD)\n",
    "\n",
    "        self.sa_heads = MultiHeadAttention(4, N_EMBD // 4)\n",
    "        self.ff = FeedForward(N_EMBD)\n",
    "        self.lm = nn.Linear(N_EMBD, vocab_size)\n",
    "\n",
    "    def predict(self, input):\n",
    "        B, T = input.shape\n",
    "        tok_embd = self.token_embedding_table(input)\n",
    "        pos_embd = self.position_embedding_table(torch.arange(T, device=device))\n",
    "        x = tok_embd + pos_embd\n",
    "\n",
    "        x = self.sa_heads(x)\n",
    "        x = self.ff(x)\n",
    "        x = self.lm(x)\n",
    "        return x\n",
    "    \n",
    "    def forward(self, input, targets):\n",
    "        logits = self.predict(input)\n",
    "        B, T, C = logits.shape\n",
    "        logits = logits.view(B*T, C)\n",
    "        targets = targets.view(B*T)\n",
    "        loss = F.cross_entropy(logits, targets)\n",
    "        return loss\n",
    "    \n",
    "    def generate(self, input, max_characters):\n",
    "        for _ in range(max_characters):\n",
    "            # Crop the input to the last BLOCK_SIZE tokens\n",
    "            input_cond = input[:, -BLOCK_SIZE:]\n",
    "\n",
    "            logits = self.predict(input_cond)\n",
    "            logits = logits[:, -1, :]\n",
    "            distribution = F.softmax(logits, dim=-1)\n",
    "            next = torch.multinomial(distribution, num_samples=1)\n",
    "            input = torch.cat((input_cond, next), dim=1)\n",
    "        return input_cond[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Blocks\n",
    "Blocks are a combination of an Attention Heads and a Feed Forward network. The idea is to chain multiple blocks to create a deep neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
    "\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(n_embd, n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out\n",
    "    \n",
    "class MultiHeadAttentionProjection(nn.Module):\n",
    "\n",
    "    def __init__(self, n_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(n_heads)])\n",
    "        self.proj = nn.Linear(N_EMBD, N_EMBD)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        x = torch.cat([head(inputs) for head in self.heads], dim=-1)\n",
    "        x = self.proj(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa = MultiHeadAttentionProjection(n_head, head_size)\n",
    "        self.ffwd = FeedForward(n_embd)\n",
    "        # self.ln1 = nn.LayerNorm(n_embd)\n",
    "        # self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    # def forward(self, x):\n",
    "    #     x = x + self.sa(self.ln1(x))\n",
    "    #     x = x + self.ffwd(self.ln2(x))\n",
    "    #     return x\n",
    "\n",
    "    def forward(self, input):\n",
    "        input = input + self.sa(input)\n",
    "        input = input + self.ffwd(input)\n",
    "\n",
    "        return input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BigramLanguageModelBlocks(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, N_EMBD)\n",
    "        self.position_embedding_table = nn.Embedding(BLOCK_SIZE, N_EMBD)\n",
    "\n",
    "        self.blocks = nn.Sequential(*[Block(N_EMBD, 4) for _ in range(3)])\n",
    "        self.lm = nn.Linear(N_EMBD, vocab_size)\n",
    "\n",
    "    def predict(self, input):\n",
    "        B, T = input.shape\n",
    "        tok_embd = self.token_embedding_table(input)\n",
    "        pos_embd = self.position_embedding_table(torch.arange(T, device=device))\n",
    "        x = tok_embd + pos_embd\n",
    "\n",
    "        x = self.blocks(x)\n",
    "        x = self.lm(x)\n",
    "        return x\n",
    "    \n",
    "    def forward(self, input, targets):\n",
    "        logits = self.predict(input)\n",
    "        B, T, C = logits.shape\n",
    "        logits = logits.view(B*T, C)\n",
    "        targets = targets.view(B*T)\n",
    "        loss = F.cross_entropy(logits, targets)\n",
    "        return loss\n",
    "    \n",
    "    def generate(self, input, max_characters):\n",
    "        for _ in range(max_characters):\n",
    "            # Crop the input to the last BLOCK_SIZE tokens\n",
    "            input_cond = input[:, -BLOCK_SIZE:]\n",
    "\n",
    "            logits = self.predict(input_cond)\n",
    "            logits = logits[:, -1, :]\n",
    "            distribution = F.softmax(logits, dim=-1)\n",
    "            next = torch.multinomial(distribution, num_samples=1)\n",
    "            input = torch.cat((input_cond, next), dim=1)\n",
    "        return input_cond[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing out all models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# models = [BigramLanguageModelSelfAttention, BigramLanguageModelMultiHeadAttention, BigramLanguageModelMultiHeadAttentionFeedForward]\n",
    "models = [BigramLanguageModelBlocks]\n",
    "context = torch.zeros((1, 1), dtype=torch.long).to(device)\n",
    "for model_cls in models:\n",
    "    model = model_cls(VOCAB_SIZE).to(device)\n",
    "    train(model, epochs=1000, eval_step=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = model.generate(context, max_characters=200)\n",
    "print(decode(res.tolist()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fyp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "faf7ff0063859ba4ac17d2ca41341beebe28095055f136698c75e8f10562c7e6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
